{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
      "0            0.2750         0.714       170200   0.817          0.000000    1   \n",
      "1            0.8770         0.838       135597   0.549          0.000964    9   \n",
      "2            0.1230         0.732       160096   0.746          0.000000    3   \n",
      "3            0.2750         0.630       203067   0.692          0.000000    7   \n",
      "4            0.1460         0.592       209449   0.572          0.000172    5   \n",
      "5            0.0519         0.828       253346   0.686          0.000002    2   \n",
      "6            0.3870         0.730       172321   0.725          0.000000   10   \n",
      "7            0.0664         0.675       216533   0.848          0.000001    1   \n",
      "8            0.5190         0.732       178000   0.583          0.000020    7   \n",
      "9            0.0498         0.758       140587   0.762          0.000004    0   \n",
      "10           0.1620         0.764       182925   0.744          0.000000   11   \n",
      "11           0.3910         0.805       181971   0.924          0.004480    7   \n",
      "12           0.3790         0.737       153308   0.750          0.000000    8   \n",
      "13           0.1870         0.812       210669   0.656          0.000000    5   \n",
      "14           0.0219         0.769       251160   0.520          0.000000    1   \n",
      "15           0.1540         0.468       259933   0.640          0.000000    0   \n",
      "16           0.2020         0.891       267267   0.714          0.000234    4   \n",
      "17           0.2840         0.778       218093   0.824          0.000000   10   \n",
      "18           0.0293         0.447       227853   0.694          0.000005    7   \n",
      "19           0.0183         0.894       250373   0.791          0.000000    2   \n",
      "20           0.0192         0.839       239987   0.804          0.000000    1   \n",
      "21           0.0928         0.918       231400   0.609          0.000000   10   \n",
      "22           0.0223         0.638       207627   0.699          0.000000    1   \n",
      "23           0.5750         0.624       288333   0.653          0.000000    8   \n",
      "24           0.0103         0.542       216933   0.853          0.000000    3   \n",
      "25           0.2830         0.865       193467   0.730          0.000000    6   \n",
      "26           0.2690         0.640       262533   0.743          0.000000   10   \n",
      "27           0.0941         0.485       242373   0.619          0.000003    5   \n",
      "28           0.0503         0.852       227547   0.604          0.000000    0   \n",
      "29           0.5950         0.686       242187   0.457          0.000000   11   \n",
      "...             ...           ...          ...     ...               ...  ...   \n",
      "10261        0.3400         0.520       258787   0.652          0.000055    5   \n",
      "10262        0.5990         0.793       211022   0.560          0.000000    7   \n",
      "10263        0.1190         0.805       238600   0.601          0.000129    0   \n",
      "10264        0.0444         0.642       207440   0.722          0.003880    2   \n",
      "10265        0.3900         0.766       231921   0.822          0.000034    1   \n",
      "10266        0.8620         0.497       216693   0.246          0.000000    7   \n",
      "10267        0.5360         0.568       415613   0.420          0.124000    7   \n",
      "10268        0.9580         0.496       198200   0.238          0.010900    1   \n",
      "10269        0.0446         0.877       231848   0.777          0.000035    1   \n",
      "10270        0.2270         0.811       134197   0.609          0.000000   10   \n",
      "10271        0.8420         0.506       257737   0.433          0.031100    2   \n",
      "10272        0.5810         0.672       217173   0.715          0.000002    0   \n",
      "10273        0.1290         0.764       251773   0.590          0.008470    7   \n",
      "10274        0.3910         0.785       357357   0.379          0.016100    7   \n",
      "10275        0.3850         0.649       142500   0.535          0.000422    6   \n",
      "10276        0.2430         0.748       262240   0.551          0.000002    1   \n",
      "10277        0.0794         0.538       281227   0.704          0.000000    1   \n",
      "10278        0.1170         0.703       265160   0.534          0.000010    0   \n",
      "10279        0.0211         0.496       336000   0.739          0.004440    6   \n",
      "10280        0.1620         0.725       247520   0.680          0.000002    8   \n",
      "10281        0.8850         0.571       240307   0.303          0.000000    4   \n",
      "10282        0.7950         0.395       226213   0.508          0.000004    7   \n",
      "10283        0.4460         0.363       174105   0.579          0.012700    7   \n",
      "10284        0.4270         0.544       274933   0.659          0.000001    9   \n",
      "10285        0.1470         0.598       256427   0.353          0.000039   10   \n",
      "10286        0.1020         0.427       214200   0.647          0.000002    6   \n",
      "10287        0.2420         0.824       217907   0.488          0.000008    9   \n",
      "10288        0.8870         0.433       244973   0.123          0.000000    1   \n",
      "10289        0.0484         0.590       169801   0.671          0.000000    9   \n",
      "10290        0.6750         0.678       197773   0.357          0.005690    0   \n",
      "\n",
      "       liveness  loudness  mode  popularity  speechiness    tempo  \\\n",
      "0        0.1400    -3.987     0          75       0.4350  174.100   \n",
      "1        0.1150    -7.145     0          81       0.0755  114.445   \n",
      "2        0.0970    -6.229     0          81       0.2750  159.881   \n",
      "3        0.1670    -4.951     0          84       0.4270  179.877   \n",
      "4        0.1150    -8.974     0          83       0.1850  164.080   \n",
      "5        0.1100    -5.242     1          84       0.2640  156.033   \n",
      "6        0.1530    -4.889     0          84       0.3180   79.648   \n",
      "7        0.1360    -3.579     0          70       0.4040  125.675   \n",
      "8        0.1080    -6.766     1          84       0.1250  100.464   \n",
      "9        0.1580    -5.428     1          66       0.3140  139.938   \n",
      "10       0.1220    -5.529     0          76       0.4480   87.538   \n",
      "11       0.0902    -5.883     0          79       0.0439  139.976   \n",
      "12       0.1310    -6.464     1          83       0.2980   81.955   \n",
      "13       0.1200    -5.976     0          73       0.1810  127.971   \n",
      "14       0.0762    -5.140     1          64       0.2120  140.112   \n",
      "15       0.1540    -5.862     1          79       0.0275   79.297   \n",
      "16       0.0521    -6.055     0          74       0.1400  100.972   \n",
      "17       0.4050    -5.892     0          83       0.0712  100.024   \n",
      "18       0.1670    -4.637     0          76       0.3820  175.868   \n",
      "19       0.0388    -4.699     1          81       0.1120  105.018   \n",
      "20       0.3310    -2.513     1          69       0.0329  122.973   \n",
      "21       0.1390    -5.640     0          81       0.0791  128.008   \n",
      "22       0.0917    -5.540     0          80       0.3840   92.939   \n",
      "23       0.1040    -6.582     0          77       0.1760   73.884   \n",
      "24       0.1080    -6.407     0          85       0.0498  105.256   \n",
      "25       0.0753    -2.813     0          74       0.3730   90.096   \n",
      "26       0.1010    -4.080     1          82       0.0379  122.035   \n",
      "27       0.1090    -7.115     0          80       0.0289  138.017   \n",
      "28       0.3270    -4.569     1          72       0.0642   94.762   \n",
      "29       0.1050    -8.322     1          85       0.0468  150.953   \n",
      "...         ...       ...   ...         ...          ...      ...   \n",
      "10261    0.2560    -6.655     1          70       0.0274   80.609   \n",
      "10262    0.0952    -8.922     0          67       0.0324  123.011   \n",
      "10263    0.0581    -9.700     0          72       0.0579   94.382   \n",
      "10264    0.1220   -13.031     1          68       0.0375  172.406   \n",
      "10265    0.3490    -4.126     0          81       0.1600  165.842   \n",
      "10266    0.1110    -7.435     1          59       0.0344   79.095   \n",
      "10267    0.1010   -11.169     0          68       0.2760   79.890   \n",
      "10268    0.0875   -11.026     1          67       0.0301  175.960   \n",
      "10269    0.0863    -4.246     1          92       0.1170  102.020   \n",
      "10270    0.1020    -8.376     1          72       0.0996  120.071   \n",
      "10271    0.1120   -14.183     0          69       0.0345   83.222   \n",
      "10272    0.1010    -6.180     1          74       0.0268  104.989   \n",
      "10273    0.0851    -4.292     1          77       0.0395   99.996   \n",
      "10274    0.2670   -11.446     1          68       0.0732  122.043   \n",
      "10275    0.1200   -12.017     0          68       0.2240   84.466   \n",
      "10276    0.0943    -9.054     1          74       0.0239  113.535   \n",
      "10277    0.0867    -8.904     0          77       0.0332  120.963   \n",
      "10278    0.1370   -11.589     1          63       0.0303   94.815   \n",
      "10279    0.1060    -7.193     1          78       0.0415  132.410   \n",
      "10280    0.1580    -5.465     1          66       0.0306  119.939   \n",
      "10281    0.0932   -10.479     0          66       0.0362  144.021   \n",
      "10282    0.0627    -3.522     1          78       0.0326   89.859   \n",
      "10283    0.2640   -10.533     1          68       0.0986   78.032   \n",
      "10284    0.1260    -7.191     1          64       0.0298  109.933   \n",
      "10285    0.0592   -15.060     0          77       0.0343  132.457   \n",
      "10286    0.1330    -8.944     0          66       0.0353   99.753   \n",
      "10287    0.1090    -9.112     1          74       0.0669  155.110   \n",
      "10288    0.1060   -11.234     0          71       0.0468  181.882   \n",
      "10289    0.0604    -6.098     1          72       0.0706  149.982   \n",
      "10290    0.1240   -14.712     0          66       0.1280  113.301   \n",
      "\n",
      "       time_signature  valence  explicit  \n",
      "0                   4   0.4080      True  \n",
      "1                   4   0.6540     False  \n",
      "2                   4   0.9270     False  \n",
      "3                   4   0.8200      True  \n",
      "4                   4   0.3150     False  \n",
      "5                   4   0.5440     False  \n",
      "6                   4   0.6250      True  \n",
      "7                   5   0.5820      True  \n",
      "8                   4   0.2770      True  \n",
      "9                   4   0.3080      True  \n",
      "10                  4   0.5070      True  \n",
      "11                  4   0.8360      True  \n",
      "12                  4   0.8570      True  \n",
      "13                  4   0.5150     False  \n",
      "14                  3   0.5800     False  \n",
      "15                  4   0.4990     False  \n",
      "16                  4   0.8180     False  \n",
      "17                  4   0.7560     False  \n",
      "18                  4   0.5090     False  \n",
      "19                  4   0.5830     False  \n",
      "20                  4   0.8880     False  \n",
      "21                  4   0.3040     False  \n",
      "22                  4   0.6600      True  \n",
      "23                  4   0.5650     False  \n",
      "24                  4   0.3700     False  \n",
      "25                  4   0.8080      True  \n",
      "26                  4   0.3610     False  \n",
      "27                  4   0.4160     False  \n",
      "28                  4   0.6670     False  \n",
      "29                  4   0.7180     False  \n",
      "...               ...      ...       ...  \n",
      "10261               4   0.7220     False  \n",
      "10262               4   0.8620     False  \n",
      "10263               4   0.7460     False  \n",
      "10264               4   0.8360     False  \n",
      "10265               4   0.8810     False  \n",
      "10266               4   0.3610     False  \n",
      "10267               4   0.4530      True  \n",
      "10268               4   0.3490     False  \n",
      "10269               4   0.7060     False  \n",
      "10270               4   0.4390      True  \n",
      "10271               4   0.6410     False  \n",
      "10272               4   0.6770     False  \n",
      "10273               4   0.2810     False  \n",
      "10274               3   0.1820     False  \n",
      "10275               4   0.8430      True  \n",
      "10276               4   0.6500     False  \n",
      "10277               4   0.5830     False  \n",
      "10278               4   0.5240     False  \n",
      "10279               4   0.3780     False  \n",
      "10280               4   0.7600     False  \n",
      "10281               4   0.3540     False  \n",
      "10282               4   0.1680     False  \n",
      "10283               4   0.7190     False  \n",
      "10284               4   0.3610     False  \n",
      "10285               4   0.3420     False  \n",
      "10286               4   0.2980     False  \n",
      "10287               4   0.8780     False  \n",
      "10288               4   0.1370     False  \n",
      "10289               4   0.4380      True  \n",
      "10290               5   0.0661     False  \n",
      "\n",
      "[10291 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "data = pd.read_csv('./datasets/relax+banger+classical+sleep+study_dataset_cleaned.csv')\n",
    "\n",
    "#x_data = data.loc[:, data.columns != \"isBanger\"]\n",
    "#x_data = data[['acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','popularity','speechiness','tempo','time_signature','valence','explicit']]\n",
    "x_data = data[['danceability', 'duration_ms', 'energy', 'key', 'loudness', 'mode', 'popularity', 'tempo', 'time_signature']]\n",
    "y_data = data.loc[:, \"isBanger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 10\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## Linear Regression ##########################\n",
      "\n",
      "0.9055948910176316 0.9002590673575129\n"
     ]
    }
   ],
   "source": [
    "print('########################## Linear Regression ##########################\\n')\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "reg_train_accuracy = accuracy_score(y_train, reg.predict(X_train).round())\n",
    "reg_test_accuracy = accuracy_score(y_test,reg.predict(X_test).round())\n",
    "print(reg_train_accuracy,reg_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "########################## Random Forest ##########################\n",
      "\n",
      "0.9014299597389976 0.9034974093264249\n",
      "explicit            0.346028\n",
      "loudness            0.182599\n",
      "instrumentalness    0.134246\n",
      "energy              0.083212\n",
      "speechiness         0.071448\n",
      "acousticness        0.067080\n",
      "danceability        0.065127\n",
      "popularity          0.026011\n",
      "valence             0.009938\n",
      "time_signature      0.006634\n",
      "tempo               0.006518\n",
      "duration_ms         0.001158\n",
      "mode                0.000000\n",
      "liveness            0.000000\n",
      "key                 0.000000\n",
      "dtype: float64\n",
      "Fitting 10 folds for each of 42 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 100, 'n_estimators': 100}\n",
      "0.9307233097320561\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n########################## Random Forest ##########################\\n')\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf_train_accuracy = accuracy_score(y_train, clf.predict(X_train).round())\n",
    "clf_test_accuracy = accuracy_score(y_test, clf.predict(X_test).round())\n",
    "print(clf_train_accuracy, clf_test_accuracy)\n",
    "\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, index=x_data.columns).sort_values(ascending=False))\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1, 3, 10, 30, 100, 300],\n",
    "    'n_estimators': [1, 3, 10, 30, 100, 300, 1000]\n",
    "}\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "########################## Support Vector Machine ##########################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abella/.local/share/virtualenvs/isitabanger/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/abella/.local/share/virtualenvs/isitabanger/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Users/abella/.local/share/virtualenvs/isitabanger/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9362765514369012 0.9229274611398963\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   26.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'kernel': 'rbf'}\n",
      "0.9257253921976955\n",
      "{'mean_fit_time': array([ 0.50843754,  2.69657161,  0.35503478,  1.08603098,  0.30984581,\n",
      "        0.56118984,  0.5435147 ,  0.59133668,  2.46578703,  0.81687722,\n",
      "       13.45910792,  1.47196095]), 'std_fit_time': array([0.03173589, 0.03788017, 0.05400447, 0.06094466, 0.01897483,\n",
      "       0.01407405, 0.02395511, 0.04193463, 0.0912971 , 0.07986721,\n",
      "       0.33243924, 0.07318635]), 'mean_score_time': array([0.02808297, 0.1612709 , 0.02137563, 0.08071251, 0.01843805,\n",
      "       0.04484797, 0.01851132, 0.04263055, 0.02204781, 0.03724051,\n",
      "       0.01312656, 0.03363314]), 'std_score_time': array([0.0039058 , 0.01901546, 0.00379577, 0.00175376, 0.00062121,\n",
      "       0.00059989, 0.00073742, 0.00496656, 0.00237497, 0.00300215,\n",
      "       0.00281202, 0.00080861]), 'param_C': masked_array(data=[0.001, 0.001, 0.01, 0.01, 0.1, 0.1, 1, 1, 10, 10, 100,\n",
      "                   100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['linear', 'rbf', 'linear', 'rbf', 'linear', 'rbf',\n",
      "                   'linear', 'rbf', 'linear', 'rbf', 'linear', 'rbf'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.001, 'kernel': 'linear'}, {'C': 0.001, 'kernel': 'rbf'}, {'C': 0.01, 'kernel': 'linear'}, {'C': 0.01, 'kernel': 'rbf'}, {'C': 0.1, 'kernel': 'linear'}, {'C': 0.1, 'kernel': 'rbf'}, {'C': 1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'linear'}, {'C': 10, 'kernel': 'rbf'}, {'C': 100, 'kernel': 'linear'}, {'C': 100, 'kernel': 'rbf'}], 'split0_test_score': array([0.90152566, 0.76837725, 0.91539528, 0.9112344 , 0.91539528,\n",
      "       0.91816921, 0.91262136, 0.91816921, 0.91262136, 0.91539528,\n",
      "       0.91262136, 0.90429958]), 'split1_test_score': array([0.92926491, 0.77531207, 0.92649098, 0.91678225, 0.92649098,\n",
      "       0.92787795, 0.92649098, 0.92926491, 0.92649098, 0.92787795,\n",
      "       0.92649098, 0.9223301 ]), 'split2_test_score': array([0.92094313, 0.7517337 , 0.92649098, 0.91955617, 0.92371706,\n",
      "       0.92094313, 0.92371706, 0.9223301 , 0.92371706, 0.92510402,\n",
      "       0.92371706, 0.90984743]), 'split3_test_score': array([0.9223301 , 0.75034674, 0.92371706, 0.91816921, 0.93065187,\n",
      "       0.9223301 , 0.92926491, 0.92371706, 0.92926491, 0.92371706,\n",
      "       0.92926491, 0.90429958]), 'split4_test_score': array([0.91527778, 0.75277778, 0.92083333, 0.92083333, 0.91944444,\n",
      "       0.92083333, 0.91666667, 0.91666667, 0.91666667, 0.92222222,\n",
      "       0.91666667, 0.91388889]), 'split5_test_score': array([0.91805556, 0.77361111, 0.93055556, 0.91805556, 0.92777778,\n",
      "       0.93055556, 0.92638889, 0.92916667, 0.92638889, 0.92916667,\n",
      "       0.92638889, 0.93194444]), 'split6_test_score': array([0.91527778, 0.75972222, 0.92222222, 0.91388889, 0.91805556,\n",
      "       0.91805556, 0.91944444, 0.91666667, 0.91944444, 0.91111111,\n",
      "       0.91944444, 0.90555556]), 'split7_test_score': array([0.93472222, 0.76666667, 0.9375    , 0.92916667, 0.93472222,\n",
      "       0.93333333, 0.93472222, 0.9375    , 0.93472222, 0.94722222,\n",
      "       0.93472222, 0.93611111]), 'split8_test_score': array([0.91527778, 0.76805556, 0.92083333, 0.91944444, 0.925     ,\n",
      "       0.92083333, 0.92222222, 0.93055556, 0.92222222, 0.925     ,\n",
      "       0.92222222, 0.91944444]), 'split9_test_score': array([0.91933241, 0.75938804, 0.92350487, 0.91376912, 0.93324061,\n",
      "       0.92767733, 0.93324061, 0.93324061, 0.93324061, 0.92489569,\n",
      "       0.93324061, 0.91655076]), 'mean_test_score': array([0.91920033, 0.76259892, 0.92475357, 0.91808968, 0.92544773,\n",
      "       0.92405942, 0.92447591, 0.92572539, 0.92447591, 0.92517007,\n",
      "       0.92447591, 0.91642371]), 'std_test_score': array([0.00845715, 0.00863788, 0.00574038, 0.0046818 , 0.00612552,\n",
      "       0.00509993, 0.00666999, 0.00692416, 0.00666999, 0.00901953,\n",
      "       0.00666999, 0.01065052]), 'rank_test_score': array([ 9, 12,  4, 10,  2,  8,  5,  1,  5,  3,  5, 11], dtype=int32), 'split0_train_score': array([0.91993212, 0.76457883, 0.92579451, 0.91808084, 0.92656587,\n",
      "       0.92749151, 0.92672015, 0.935668  , 0.92610305, 0.95726628,\n",
      "       0.92625733, 0.97670472]), 'split1_train_score': array([0.92054921, 0.76180191, 0.92533169, 0.91977785, 0.92456032,\n",
      "       0.92625733, 0.92533169, 0.93613082, 0.92502314, 0.95294662,\n",
      "       0.92517741, 0.97269361]), 'split2_train_score': array([0.92070349, 0.76457883, 0.92517741, 0.91993212, 0.92656587,\n",
      "       0.92625733, 0.92718297, 0.93690219, 0.92702869, 0.95464363,\n",
      "       0.92702869, 0.97500771]), 'split3_train_score': array([0.92039494, 0.76689293, 0.92594878, 0.91993212, 0.9264116 ,\n",
      "       0.92749151, 0.92533169, 0.93736501, 0.92579451, 0.95510645,\n",
      "       0.92579451, 0.97531626]), 'split4_train_score': array([0.92225821, 0.76708314, 0.92642295, 0.91963597, 0.9265772 ,\n",
      "       0.92611445, 0.9262687 , 0.93691192, 0.92642295, 0.95619312,\n",
      "       0.92642295, 0.97717106]), 'split5_train_score': array([0.92179547, 0.76045041, 0.9256517 , 0.91932747, 0.9256517 ,\n",
      "       0.92611445, 0.92549745, 0.93552368, 0.92549745, 0.95387938,\n",
      "       0.92549745, 0.97655406]), 'split6_train_score': array([0.92148697, 0.7653864 , 0.92673145, 0.91979022, 0.92642295,\n",
      "       0.9278112 , 0.9268857 , 0.93598643, 0.92673145, 0.95588462,\n",
      "       0.92673145, 0.97470307]), 'split7_train_score': array([0.91886472, 0.75967916, 0.92441771, 0.91840197, 0.92457196,\n",
      "       0.92549745, 0.92488046, 0.93552368, 0.92488046, 0.95341663,\n",
      "       0.92472621, 0.97485732]), 'split8_train_score': array([0.92164122, 0.76245565, 0.9265772 , 0.91963597, 0.9265772 ,\n",
      "       0.92703995, 0.9265772 , 0.93583218, 0.92642295, 0.95480487,\n",
      "       0.9262687 , 0.97516582]), 'split9_train_score': array([0.92072795, 0.76573103, 0.92489204, 0.91964837, 0.92550895,\n",
      "       0.92612585, 0.92520049, 0.93476249, 0.92535472, 0.95434917,\n",
      "       0.92535472, 0.97516965]), 'mean_train_score': array([0.92083543, 0.76386383, 0.92569455, 0.91941629, 0.92594136,\n",
      "       0.9266201 , 0.92598765, 0.93606064, 0.92592594, 0.95484908,\n",
      "       0.92592594, 0.97533433]), 'std_train_score': array([0.00094997, 0.00248523, 0.00071657, 0.00061427, 0.00078071,\n",
      "       0.00073436, 0.00078388, 0.00074919, 0.00069466, 0.00125274,\n",
      "       0.00069757, 0.00120421])}\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n########################## Support Vector Machine ##########################\\n')\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "clf_train_accuracy = accuracy_score(y_train, clf.predict(X_train_scaled).round())\n",
    "clf_test_accuracy = accuracy_score(y_test, clf.predict(X_test_scaled).round())\n",
    "print(clf_train_accuracy, clf_test_accuracy)\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "rf = SVC(gamma='auto')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "########################## Principal Component Analysis ##########################\n",
      "\n",
      "[9.99999884e-01 9.36448309e-08 1.81788038e-08 3.29489794e-09\n",
      " 1.22518811e-09 2.33297998e-11 2.07640534e-11 1.87453660e-11\n",
      " 7.42930047e-12 4.72745665e-12]\n",
      "[1.04812866e+07 3.20742514e+03 1.41317932e+03 6.01638482e+02\n",
      " 3.66873219e+02 5.06255937e+01 4.77607009e+01 4.53796963e+01\n",
      " 2.85685754e+01 2.27891644e+01]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n########################## Principal Component Analysis ##########################\\n')\n",
    "\n",
    "pca = PCA(n_components=10, svd_solver='full')\n",
    "pca.fit(x_data)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
